{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "974226a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jongholee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65f2cae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████████████████████| 1.97k/1.97k [00:00<00:00, 810kB/s]\n",
      "Downloading: 100%|███████████████████████████| 792k/792k [00:00<00:00, 4.65MB/s]\n",
      "Downloading: 100%|██████████████████████████| 1.79k/1.79k [00:00<00:00, 320kB/s]\n",
      "Downloading: 100%|██████████████████████████| 1.39k/1.39k [00:00<00:00, 310kB/s]\n",
      "Downloading: 100%|███████████████████████████| 242M/242M [00:05<00:00, 40.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"hetpandya/t5-small-tapaco\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"hetpandya/t5-small-tapaco\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a62ac80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function for the paraphrase\n",
    "def my_paraphrase(sentence):\n",
    "   \n",
    "    sentence = \"paraphrase: \" + sentence + \" </s>\"\n",
    "    encoding = tokenizer.encode_plus(sentence,padding=True, return_tensors=\"pt\")\n",
    "    input_ids, attention_masks = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
    "\n",
    "    outputs = model.generate(\n",
    "    input_ids=input_ids, attention_mask=attention_masks,\n",
    "    do_sample=True, #샘플링 전략 사용\n",
    "    max_length=50, # 최대 디코딩 길이는 50\n",
    "    top_k=200, # 확률 순위가 50위 밖인 토큰은 샘플링에서 제외\n",
    "    top_p=0.98, # 누적 확률이 95%인 후보집합에서만 생성\n",
    "    num_return_sequences=5) #3개의 결과를 디코딩해낸다\n",
    "\n",
    "    output = tokenizer.decode(outputs[0], skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e419abde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def StartEdit():\n",
    "    print(\"-----------------\")\n",
    "    print(\"Enter the essay you want to paraphrase.\")\n",
    "\n",
    "    draftEssay_input = input()\n",
    "    \n",
    "    print(\"processing...\")\n",
    "\n",
    "\n",
    "    ExceptionInput(draftEssay_input)\n",
    "\n",
    "    result_essay_ai = \" \".join([my_paraphrase(sent) for sent in sent_tokenize(draftEssay_input)])\n",
    "    \n",
    "    print(\"Result : \", result_essay_ai)\n",
    "    \n",
    "    return result_essay_ai #  selected_final_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ada64aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExceptionInput(input_txt):\n",
    "    if (input_txt.isdigit()):\n",
    "        print(\"An incorrect input was detected. Enter your essay and try again.\")\n",
    "        StartEdit()\n",
    "\n",
    "    elif (len(input_txt) <= 20):\n",
    "        print(\"The sentence is too short. Enter your essay and try again.\")\n",
    "        StartEdit()\n",
    "\n",
    "    elif (input_txt == None):\n",
    "        print(\"Enter the essay you want to paraphrase.\")\n",
    "        StartEdit()\n",
    "\n",
    "\n",
    "    return input_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670f3f2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Enter the essay you want to paraphrase.\n"
     ]
    }
   ],
   "source": [
    "# run \n",
    "# input : fucking ridiculous bullshit\n",
    "\n",
    "StartEdit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cf4c4f",
   "metadata": {},
   "source": [
    "욕설이 감지되면 화가난 정도를 분석한 후 적합한 코멘트를 제시해주는 AI generation 기능을 구현한다. 표현 DB를 만드는 것이 중요함.\n",
    "비폭력적 대화를 생성하는 ai로 알고리즘 구현을 하자\n",
    "\n",
    "1)관찰\n",
    "2)느낌(주의할 점은 평가하지 않는다. 평가를 하면 오해와 불쾌감을 유발한다.\n",
    "3)부탁\n",
    "\n",
    "서로 존중해주길 원하는데 그렇게 당신이 말하면 모욕적으로 들려서 내가 마음이 불편하다.\n",
    "네가 집에 돌아오면 얼굴을 마주하고 인사해줬으면 좋겠어.\n",
    "지금의 속도가 좀 빠른 거 같아서, 불안한데.\n",
    "사고라도 날까봐 걱정돼서 그러는 구나.\n",
    "안전하게 편안한 마음으로 가고 싶거든, 경치 구경도 여유롭게 하고 말이야.\n",
    "편한 마음으로 즐기면서 가고 싶은 거구나.\n",
    "바로 그거야."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2382c5ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
